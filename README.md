# ğŸ§  Trading Strategy Optimization with PPO + Optuna

Este proyecto utiliza **Aprendizaje por Refuerzo (RL)** con el algoritmo **PPO (Proximal Policy Optimization)** de la librerÃ­a `stable-baselines3`, aplicado a un entorno personalizado de trading. La bÃºsqueda de los mejores parÃ¡metros de recompensa se realiza automÃ¡ticamente con **Optuna**, una potente herramienta de optimizaciÃ³n bayesiana.

## ğŸ“ˆ Objetivo

Entrenar un agente que aprenda a comprar, vender o mantenerse inactivo en funciÃ³n de datos de snapshots del orderbook, maximizando su **equity final** bajo condiciones de mercado reales.

## ğŸ§° Estructura del Proyecto

```
project/
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â”œâ”€â”€ models_final/
â”œâ”€â”€ env_simple.py
â”œâ”€â”€ optimize_refined.py
â”œâ”€â”€ optimize_final.py
â”œâ”€â”€ evaluate_best_configs.py
â”œâ”€â”€ optuna_final_results.csv
â””â”€â”€ README.md
```

## âš™ï¸ TecnologÃ­as

- [Stable-Baselines3 (PPO)](https://github.com/DLR-RM/stable-baselines3)
- [Optuna](https://optuna.org/)
- [Gymnasium](https://gymnasium.farama.org/)
- [Pandas, NumPy]

## ğŸš€ Â¿CÃ³mo correr la optimizaciÃ³n?

### 1. InstalÃ¡ dependencias

```bash
pip install stable-baselines3 optuna gym pandas numpy
```

### 2. EjecutÃ¡ una optimizaciÃ³n refinada

```bash
python optimize_final.py
```

### 3. Evaluar consistencia de los mejores modelos

```bash
python evaluate_best_configs.py
```

## ğŸ“Š Dashboard en tiempo real

```bash
pip install optuna-dashboard
optuna-dashboard sqlite:///final_optuna_study.db
```

AbrÃ­ en tu navegador:

```
http://localhost:8080
```

## ğŸ§  Â¿QuÃ© se optimiza?

- `reward_trade`
- `reward_hold`
- `reward_profit`
- `reward_loss`
- `reward_idle`

El entorno tambiÃ©n usa el coeficiente de Hurst, el exponente de Lyapunov y volatilidad local.

## ğŸ“¬ Contacto

Proyecto creado y entrenado por [Tu Nombre o Usuario].
